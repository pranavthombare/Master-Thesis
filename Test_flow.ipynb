{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from fr_utils import *\n",
    "from faceDetector import *\n",
    "from inception_blocks_v2 import *\n",
    "from parameters import *\n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Conv2D,ZeroPadding2D,Activation,Input,concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D,AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda,Flatten,Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def recognize_face(image_path,database,model):\\n    encoding = img_to_encoding(image_path,model)\\n    min_dist = 100\\n    for(name , db_encoding) in database.items():\\n        dist = np.linalg.norm(encoding - database[name])\\n        if dist<min_dist:\\n            min_dist = dist\\n            identity = name\\n    if min_dist > 0.7:\\n        print(\"Intruder,distance is \" + str(min_dist))\\n    else:\\n        print(\"Welcome \" +str(identity) +\",distance is \" + str(min_dist))\\n    return min_dist,identity\\n\\ndef img_to_encoding(image_path, model):\\n    img1 = cv2.imread(image_path, 1)\\n    img = img1[...,::-1]\\n\\n    classifier = cv2.CascadeClassifier(\\'haarcascade_frontalface_default.xml\\')\\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n    faces = classifier.detectMultiScale(gray, 1.3, 5)\\n    print(\"Found {0} face(s)!\".format(len(faces)))\\n    plt.figure(1)\\n    for (x, y, w, h) in faces:\\n        cv2.rectangle(img1, (x-10, y-10), (x+w+10, y+h+10), (0, 255, 0), 2)\\n        img=img[y-10:y+h+10,x-10:x+w+10]\\n        face_detected = Image.fromarray(img1,\\'RGB\\')\\n        plt.subplot(121)\\n        plt.title(\"Face detected image\")\\n        plt.imshow(np.asarray(face_detected))\\n\\n    crp_image = Image.fromarray(img,\\'RGB\\')\\n    plt.subplot(122)\\n    plt.title(\"Cropped image\")\\n    plt.imshow(np.asarray(crp_image))\\n    plt.show()\\n\\n    img = cv2.resize(img,(96,96))\\n    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\\n    x_train = np.array([img])\\n    #embedding = model.predict_on_batch(x_train)\\n    #return embedding\\n\\n    def show_image(true_image,test_image):\\n        img1 = cv2.imread(true_image)\\n        b,g,r = cv2.split(img1)\\n        img1 = cv2.merge([r,g,b])\\n        plt.figure(1)\\n        plt.subplot(121)\\n        plt.title(\"True Image\")\\n        plt.imshow(img1)\\n        img2 = cv2.imread(test_image)\\n        b,g,r = cv2.split(img2)\\n        img2 = cv2.merge([r,g,b])\\n        plt.subplot(122)\\n        plt.title(\"Test Image\")\\n        plt.imshow(img2)\\n        plt.show()'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def load_model():\n",
    "    model = faceRecoModel(input_shape = (3,96,96))\n",
    "    model.compile(optimizer= 'adam',loss= triplet_loss,metrics = ['accuracy'])\n",
    "    load_weights_from_FaceNet(model)\n",
    "    print(\"Model loaded\")\n",
    "    return model\n",
    "'''\n",
    "\n",
    "def triplet_loss(y_true, y_pred, alpha = ALPHA):\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))    \n",
    "    return loss\n",
    "\n",
    "'''def add_face(name):\n",
    "    os.mkdir(\"images/\" + str(name))\n",
    "    cam = cv2.VideoCapture(0)\n",
    "\n",
    "    #we'll use the 'haarcascade_frontalface_default.xml' which is in the same folder as this code\n",
    "    face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # For each person, enter one numeric face id. for now this will be used to make unique ID\n",
    "\n",
    "    print(\"\\n [INFO] Initializing face capture. Look the camera and wait ...\")\n",
    "    # Initialize individual sampling face count\n",
    "    count = 0\n",
    "\n",
    "    #start detect your face and take 30 pictures\n",
    "    while(True):\n",
    "\n",
    "        ret, img = cam.read()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detector.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x,y,w,h) in faces:\n",
    "\n",
    "            cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "            count += 1\n",
    "\n",
    "            # Save the captured image into the datasets folder\n",
    "            cv2.imwrite(\"images/\" + str(name) +\"/\" + str(name) + '_' + str(count) + \".jpg\", gray[y:y+h,x:x+w])\n",
    "\n",
    "            cv2.imshow('face', img)\n",
    "\n",
    "        k = cv2.waitKey(100) & 0xff # Press 'ESC' for exiting video\n",
    "        if k == 27:\n",
    "            break\n",
    "        elif count >= 5: # Take 5 face sample and stop video\n",
    "             break\n",
    "\n",
    "    # Do a bit of cleanup\n",
    "    print(\"\\n [INFO] Exiting Program and cleanup stuff\")\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()'''\n",
    "    \n",
    "def add_face_database(image_path,name,model):\n",
    "    database[name] = img_to_encoding(image_path,model)\n",
    "    return str(name) + \" added\"\n",
    "\n",
    "def verify(image_path, identity, database, model):\n",
    "    \n",
    "    encoding = img_to_encoding(image_path, model, False)\n",
    "    min_dist = 1000\n",
    "    for  pic in database:\n",
    "        dist = np.linalg.norm(encoding - pic)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "    print(identity + ' : ' +str(min_dist)+ ' ' + str(len(database)))\n",
    "    \n",
    "    if min_dist<THRESHOLD:\n",
    "        door_open = True\n",
    "    else:\n",
    "        door_open = False\n",
    "        \n",
    "    return min_dist, door_open\n",
    "\n",
    "'''def recognize_face(image_path,database,model):\n",
    "    encoding = img_to_encoding(image_path,model)\n",
    "    min_dist = 100\n",
    "    for(name , db_encoding) in database.items():\n",
    "        dist = np.linalg.norm(encoding - database[name])\n",
    "        if dist<min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "    if min_dist > 0.7:\n",
    "        print(\"Intruder,distance is \" + str(min_dist))\n",
    "    else:\n",
    "        print(\"Welcome \" +str(identity) +\",distance is \" + str(min_dist))\n",
    "    return min_dist,identity\n",
    "\n",
    "def img_to_encoding(image_path, model):\n",
    "    img1 = cv2.imread(image_path, 1)\n",
    "    img = img1[...,::-1]\n",
    "\n",
    "    classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    print(\"Found {0} face(s)!\".format(len(faces)))\n",
    "    plt.figure(1)\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img1, (x-10, y-10), (x+w+10, y+h+10), (0, 255, 0), 2)\n",
    "        img=img[y-10:y+h+10,x-10:x+w+10]\n",
    "        face_detected = Image.fromarray(img1,'RGB')\n",
    "        plt.subplot(121)\n",
    "        plt.title(\"Face detected image\")\n",
    "        plt.imshow(np.asarray(face_detected))\n",
    "\n",
    "    crp_image = Image.fromarray(img,'RGB')\n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Cropped image\")\n",
    "    plt.imshow(np.asarray(crp_image))\n",
    "    plt.show()\n",
    "\n",
    "    img = cv2.resize(img,(96,96))\n",
    "    img = np.around(np.transpose(img, (2,0,1))/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    #embedding = model.predict_on_batch(x_train)\n",
    "    #return embedding\n",
    "\n",
    "    def show_image(true_image,test_image):\n",
    "        img1 = cv2.imread(true_image)\n",
    "        b,g,r = cv2.split(img1)\n",
    "        img1 = cv2.merge([r,g,b])\n",
    "        plt.figure(1)\n",
    "        plt.subplot(121)\n",
    "        plt.title(\"True Image\")\n",
    "        plt.imshow(img1)\n",
    "        img2 = cv2.imread(test_image)\n",
    "        b,g,r = cv2.split(img2)\n",
    "        img2 = cv2.merge([r,g,b])\n",
    "        plt.subplot(122)\n",
    "        plt.title(\"Test Image\")\n",
    "        plt.imshow(img2)\n",
    "        plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recog(predict_image,database,model):\n",
    "    recognize_face(predict_image,database,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(image_path, identity, database, model):\n",
    "    \n",
    "    encoding = img_to_encoding(image_path, model, False)\n",
    "    min_dist = 1000\n",
    "    for  pic in database:\n",
    "        dist = np.linalg.norm(encoding - pic)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "    print(identity + ' : ' +str(min_dist)+ ' ' + str(len(database)))\n",
    "    \n",
    "    if min_dist<THRESHOLD:\n",
    "        door_open = True\n",
    "    else:\n",
    "        door_open = False\n",
    "        \n",
    "    return min_dist, door_open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_image('images/fnameTrue.jpg','images/fnameTest.jpg')\n",
    "#add_face_database(\"images/s.jpg\",\"Sachin\",model)\n",
    "add_face(\"pranav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_face_database(\"images/pranav/pranav_1.jpg\",\"Pranav\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image('images/pranav/pranav_1.jpg','images/pranav/pranav_2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_encoding(\"images/pranav/pranav_1.jpg\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog(\"images/TC.jpg\",database,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path =\"\"\n",
    "if(os.path.exists(\"bestmodel.txt\")):\n",
    "    with open('bestmodel.txt', 'r') as file:\n",
    "        best_model_path = file.read()\n",
    "    \n",
    "with open(\"./path_dict.p\", 'rb') as f:\n",
    "    paths = pickle.load(f)\n",
    "    \n",
    "faces = []\n",
    "for key in paths.keys():\n",
    "    paths[key] = paths[key].replace(\"\\\\\", \"/\")\n",
    "    faces.append(key)\n",
    "    \n",
    "if(len(faces) == 0):\n",
    "    print(\"No images found in database!!\")\n",
    "    print(\"Please add images to database\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom trained model not found, Loading original facenet model...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a8f3c92aa38f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Custom trained model not found, Loading original facenet model...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mFRmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaceRecoModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMAGE_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mload_weights_from_FaceNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFRmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Untitled Folder\\inception_blocks_v2.py\u001b[0m in \u001b[0;36mfaceRecoModel\u001b[1;34m(input_shape)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minception_block_1a\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minception_block_1b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minception_block_1c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;31m# Inception 2: a/b\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Untitled Folder\\inception_blocks_v2.py\u001b[0m in \u001b[0;36minception_block_1c\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     86\u001b[0m                            \u001b[0mcv2_filter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                            \u001b[0mcv2_strides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                            padding=(1, 1))\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     X_5x5 = fr_utils.conv2d_bn(X,\n",
      "\u001b[1;32m~\\Desktop\\Untitled Folder\\fr_utils.py\u001b[0m in \u001b[0;36mconv2d_bn\u001b[1;34m(x, layer, cv1_out, cv1_filter, cv1_strides, cv2_out, cv2_filter, cv2_strides, padding)\u001b[0m\n\u001b[0;32m     53\u001b[0m               padding=None):\n\u001b[0;32m     54\u001b[0m     \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcv2_out\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv1_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv1_filter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv1_strides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'channels_first'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_conv'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_bn'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    872\u001b[0m               \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mask'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m             inputs, outputs = self._set_connectivity_metadata_(\n\u001b[1;32m--> 874\u001b[1;33m                 inputs, outputs, args, kwargs)\n\u001b[0m\u001b[0;32m    875\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_set_connectivity_metadata_\u001b[1;34m(self, inputs, outputs, args, kwargs)\u001b[0m\n\u001b[0;32m   2036\u001b[0m     \u001b[1;31m# This updates the layer history of the output tensor(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2037\u001b[0m     self._add_inbound_node(\n\u001b[1;32m-> 2038\u001b[1;33m         input_tensors=inputs, output_tensors=outputs, arguments=arguments)\n\u001b[0m\u001b[0;32m   2039\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_add_inbound_node\u001b[1;34m(self, input_tensors, output_tensors, arguments)\u001b[0m\n\u001b[0;32m   2052\u001b[0m     \"\"\"\n\u001b[0;32m   2053\u001b[0m     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[1;32m-> 2054\u001b[1;33m                                         input_tensors)\n\u001b[0m\u001b[0;32m   2055\u001b[0m     node_indices = nest.map_structure(lambda t: t._keras_history.node_index,\n\u001b[0;32m   2056\u001b[0m                                       input_tensors)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 535\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 535\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   2051\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2052\u001b[0m     \"\"\"\n\u001b[1;32m-> 2053\u001b[1;33m     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[0m\u001b[0;32m   2054\u001b[0m                                         input_tensors)\n\u001b[0;32m   2055\u001b[0m     node_indices = nest.map_structure(lambda t: t._keras_history.node_index,\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'layer'"
     ]
    }
   ],
   "source": [
    "if os.path.exists(best_model_path) and best_model_path !=\"\":\n",
    "    print(\"Trained model found\")\n",
    "    print(\"Loading custom trained model...\")\n",
    "    FRmodel = keras.models.load_model(best_model_path,custom_objects={'triplet_loss': triplet_loss})\n",
    "    \n",
    "else:\n",
    "    print(\"Custom trained model not found, Loading original facenet model...\")\n",
    "    FRmodel = faceRecoModel(input_shape=(3, IMAGE_SIZE, IMAGE_SIZE))\n",
    "    load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}\n",
    "for face in faces:\n",
    "    database[face] = []\n",
    "\n",
    "for face in faces:\n",
    "    for img in os.listdir(paths[face]):\n",
    "        database[face].append(img_to_encoding(os.path.join(paths[face],img), FRmodel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "fd = faceDetector('fd_models/haarcascade_frontalface_default.xml')\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') #codec for video\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20, (800, 600) )#Output object\n",
    "\n",
    "while True:\n",
    "    ret, frame = camera.read()\n",
    "    frame = imutils.resize(frame, width = 800)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    print(frame.shape)\n",
    "    faceRects = fd.detect(gray)\n",
    "    for (x, y, w, h) in faceRects:\n",
    "        roi = frame[y:y+h,x:x+w]\n",
    "        roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "        roi = cv2.resize(roi,(IMAGE_SIZE, IMAGE_SIZE))\n",
    "        min_dist = 1000\n",
    "        identity = \"\"\n",
    "        detected  = False\n",
    "        \n",
    "        for face in range(len(faces)):\n",
    "            person = faces[face]\n",
    "            dist, detected = verify(roi, person, database[person], FRmodel)\n",
    "            if detected == True and dist<min_dist:\n",
    "                min_dist = dist\n",
    "                identity = person\n",
    "        if detected == True:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, identity, (x+ (w//2),y-2), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), lineType=cv2.LINE_AA)\n",
    "            \n",
    "    cv2.imshow('frame', frame)\n",
    "    out.write(frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
